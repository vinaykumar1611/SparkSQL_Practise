{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh_Do23DXwi-",
        "outputId": "7e2c044a-5290-46e8-dc4e-812c4bace3a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.3.2.tar.gz (281.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py4j==0.10.9.5\n",
            "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.3.2-py2.py3-none-any.whl size=281824025 sha256=1372b35e832e68b56f861b76a22d4c8810d02b6853f404ba15f34d8963be8c5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/59/a0/a1a0624b5e865fd389919c1a10f53aec9b12195d6747710baf\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.5 pyspark-3.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf\n",
        "from pyspark.sql.types import *"
      ],
      "metadata": {
        "id": "IadUjtVVX6CT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_conf = SparkConf()"
      ],
      "metadata": {
        "id": "tWjUH9VvYQpF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_conf.set(\"My.App.Name\", \"Date_Practise\")\n",
        "my_conf.set(\"master\", \"local[*]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BK2-xvpYVBu",
        "outputId": "3817c606-20f8-49e8-e96c-3ffcbad577a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.conf.SparkConf at 0x7f37025baca0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.config(conf=my_conf).getOrCreate()"
      ],
      "metadata": {
        "id": "DNcHRdUFYmC3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputDF = spark.read.csv(\"/content/biglog.txt\",header=True)"
      ],
      "metadata": {
        "id": "aZD1Kn3bY0Kb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDPW9VyBZYMK",
        "outputId": "e66d8976-3422-43c0-ca43-67280645f22c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------+\n",
            "|level|           datetime|\n",
            "+-----+-------------------+\n",
            "|DEBUG|  2015-2-6 16:24:07|\n",
            "| WARN| 2016-7-26 18:54:43|\n",
            "| INFO|2012-10-18 14:35:19|\n",
            "|DEBUG| 2012-4-26 14:26:50|\n",
            "|DEBUG| 2013-9-28 20:27:13|\n",
            "| INFO| 2017-8-20 13:17:27|\n",
            "| INFO| 2015-4-13 09:28:17|\n",
            "|DEBUG| 2015-7-17 00:49:27|\n",
            "|DEBUG| 2014-7-26 02:33:09|\n",
            "| INFO| 2016-1-13 09:51:57|\n",
            "|DEBUG| 2015-1-14 08:55:30|\n",
            "|DEBUG| 2016-1-20 03:47:06|\n",
            "|DEBUG|  2013-7-8 21:00:50|\n",
            "|DEBUG| 2012-5-22 11:43:57|\n",
            "|DEBUG| 2013-3-20 06:14:50|\n",
            "| INFO|  2015-8-8 20:49:22|\n",
            "| WARN| 2015-1-14 20:05:00|\n",
            "| INFO| 2017-6-14 00:08:35|\n",
            "| INFO| 2016-1-18 11:50:14|\n",
            "|DEBUG|  2017-7-1 12:55:02|\n",
            "+-----+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputDF.createOrReplaceTempView(\"logging\")"
      ],
      "metadata": {
        "id": "Xm4N14n4ZcZL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from logging\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVNw9wE3aH8U",
        "outputId": "73c36ed3-b8a6-4690-eb6a-a1a8110208a6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------------+\n",
            "|level|           datetime|\n",
            "+-----+-------------------+\n",
            "|DEBUG|  2015-2-6 16:24:07|\n",
            "| WARN| 2016-7-26 18:54:43|\n",
            "| INFO|2012-10-18 14:35:19|\n",
            "|DEBUG| 2012-4-26 14:26:50|\n",
            "|DEBUG| 2013-9-28 20:27:13|\n",
            "| INFO| 2017-8-20 13:17:27|\n",
            "| INFO| 2015-4-13 09:28:17|\n",
            "|DEBUG| 2015-7-17 00:49:27|\n",
            "|DEBUG| 2014-7-26 02:33:09|\n",
            "| INFO| 2016-1-13 09:51:57|\n",
            "|DEBUG| 2015-1-14 08:55:30|\n",
            "|DEBUG| 2016-1-20 03:47:06|\n",
            "|DEBUG|  2013-7-8 21:00:50|\n",
            "|DEBUG| 2012-5-22 11:43:57|\n",
            "|DEBUG| 2013-3-20 06:14:50|\n",
            "| INFO|  2015-8-8 20:49:22|\n",
            "| WARN| 2015-1-14 20:05:00|\n",
            "| INFO| 2017-6-14 00:08:35|\n",
            "| INFO| 2016-1-18 11:50:14|\n",
            "|DEBUG|  2017-7-1 12:55:02|\n",
            "+-----+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clubbedDF = spark.sql(\"select level,date_format(datetime,'MMMM') as Month from logging\")"
      ],
      "metadata": {
        "id": "l8gFvVRwZoKS"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clubbedDF.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzWg0Pt0aRML",
        "outputId": "0e5e6502-b427-40ac-eddf-369a270a0727"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+\n",
            "|level|    Month|\n",
            "+-----+---------+\n",
            "|DEBUG| February|\n",
            "| WARN|     July|\n",
            "| INFO|  October|\n",
            "|DEBUG|    April|\n",
            "|DEBUG|September|\n",
            "| INFO|   August|\n",
            "| INFO|    April|\n",
            "|DEBUG|     July|\n",
            "|DEBUG|     July|\n",
            "| INFO|  January|\n",
            "|DEBUG|  January|\n",
            "|DEBUG|  January|\n",
            "|DEBUG|     July|\n",
            "|DEBUG|      May|\n",
            "|DEBUG|    March|\n",
            "| INFO|   August|\n",
            "| WARN|  January|\n",
            "| INFO|     June|\n",
            "| INFO|  January|\n",
            "|DEBUG|     July|\n",
            "+-----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clubbedDF.createOrReplaceTempView(\"logging_level\")"
      ],
      "metadata": {
        "id": "LFa0SoknaZhW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from logging_level\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxLSw5Gpahsx",
        "outputId": "df619668-b860-4f0b-9a19-bfbab3748af9"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+\n",
            "|level|    Month|\n",
            "+-----+---------+\n",
            "|DEBUG| February|\n",
            "| WARN|     July|\n",
            "| INFO|  October|\n",
            "|DEBUG|    April|\n",
            "|DEBUG|September|\n",
            "| INFO|   August|\n",
            "| INFO|    April|\n",
            "|DEBUG|     July|\n",
            "|DEBUG|     July|\n",
            "| INFO|  January|\n",
            "|DEBUG|  January|\n",
            "|DEBUG|  January|\n",
            "|DEBUG|     July|\n",
            "|DEBUG|      May|\n",
            "|DEBUG|    March|\n",
            "| INFO|   August|\n",
            "| WARN|  January|\n",
            "| INFO|     June|\n",
            "| INFO|  January|\n",
            "|DEBUG|     July|\n",
            "+-----+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select level,Month,count(*) from logging_level group by level,month\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvd4SEL_amvN",
        "outputId": "3eb6462e-ebfd-480d-80f0-0bba54b25a24"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+--------+\n",
            "|level|    Month|count(1)|\n",
            "+-----+---------+--------+\n",
            "| WARN|     June|    8191|\n",
            "| INFO|     June|   29143|\n",
            "|ERROR| November|    3389|\n",
            "|FATAL|  January|      94|\n",
            "| WARN| December|    8328|\n",
            "| WARN|    March|    8165|\n",
            "|DEBUG|     July|   42085|\n",
            "|ERROR|    April|    4107|\n",
            "|ERROR|  January|    4054|\n",
            "|FATAL|September|      81|\n",
            "|FATAL|    April|      83|\n",
            "| INFO|September|   29038|\n",
            "|FATAL| November|   16797|\n",
            "|FATAL|  October|      92|\n",
            "| INFO| February|   28983|\n",
            "| WARN|    April|    8277|\n",
            "|DEBUG| December|   41749|\n",
            "|FATAL| December|      94|\n",
            "| WARN|      May|    8403|\n",
            "|ERROR|     June|    4059|\n",
            "+-----+---------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select level,date_format(datetime,'MMMM') as Month, count(*) as count from logging group by level, Month order by Month\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhRu1eGEa0U7",
        "outputId": "72539275-ae0d-4440-b112-087261b46b20"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+-----+\n",
            "|level|   Month|count|\n",
            "+-----+--------+-----+\n",
            "| WARN|   April| 8277|\n",
            "|ERROR|   April| 4107|\n",
            "| INFO|   April|29302|\n",
            "|FATAL|   April|   83|\n",
            "|DEBUG|   April|41869|\n",
            "|FATAL|  August|   80|\n",
            "| WARN|  August| 8381|\n",
            "| INFO|  August|28993|\n",
            "|DEBUG|  August|42147|\n",
            "|ERROR|  August| 3987|\n",
            "|DEBUG|December|41749|\n",
            "|FATAL|December|   94|\n",
            "|ERROR|December| 4106|\n",
            "| INFO|December|28874|\n",
            "| WARN|December| 8328|\n",
            "| INFO|February|28983|\n",
            "|ERROR|February| 4013|\n",
            "|DEBUG|February|41734|\n",
            "|FATAL|February|   72|\n",
            "| WARN|February| 8266|\n",
            "+-----+--------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here when i do order by it is working but i wanted it as month wise like january,febraury,March...\n",
        "# how to do it ?\n",
        "# What if i exact the month number and sort based on the month number does that work?\n",
        "# How to extract month number from date in sql?"
      ],
      "metadata": {
        "id": "_PVk7-F4nGPo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select level,date_format(datetime,'MMMM') as Month, \n",
        "date_format(datetime,'M') as Monthnum, \n",
        "count(*) as count \n",
        "from logging \n",
        "group by level, Month \n",
        "order by Month\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "hEEp2mtKntBa",
        "outputId": "963f50d5-f01c-4b63-a6e9-54019088d057"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-17f1434a55ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m spark.sql(\"\"\"select level,date_format(datetime,'MMMM') as Month, \n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'M'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mMonthnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0mby\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMonth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0msqlQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: expression 'logging.datetime' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;\nSort [Month#108 ASC NULLS FIRST], true\n+- Aggregate [level#17, date_format(cast(datetime#18 as timestamp), MMMM, Some(Etc/UTC))], [level#17, date_format(cast(datetime#18 as timestamp), MMMM, Some(Etc/UTC)) AS Month#108, date_format(cast(datetime#18 as timestamp), M, Some(Etc/UTC)) AS Monthnum#109, count(1) AS count#110L]\n   +- SubqueryAlias logging\n      +- View (`logging`, [level#17,datetime#18])\n         +- Relation [level#17,datetime#18] csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select level,date_format(datetime,'MMMM') as Month, \n",
        "first(date_format(datetime,'M')) as Monthnum, \n",
        "count(*) as count \n",
        "from logging \n",
        "group by level, Month \n",
        "order by Monthnum\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3EmdcRkoDWN",
        "outputId": "13e34c67-51da-43ed-8de2-633fc7604950"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+--------+-----+\n",
            "|level|   Month|Monthnum|count|\n",
            "+-----+--------+--------+-----+\n",
            "|FATAL| January|       1|   94|\n",
            "|DEBUG| January|       1|41961|\n",
            "| INFO| January|       1|29119|\n",
            "|ERROR| January|       1| 4054|\n",
            "| WARN| January|       1| 8217|\n",
            "|DEBUG| October|      10|41936|\n",
            "| WARN| October|      10| 8226|\n",
            "|ERROR| October|      10| 4040|\n",
            "| INFO| October|      10|29018|\n",
            "|FATAL| October|      10|   92|\n",
            "|DEBUG|November|      11|33366|\n",
            "|FATAL|November|      11|16797|\n",
            "| INFO|November|      11|23301|\n",
            "| WARN|November|      11| 6616|\n",
            "|ERROR|November|      11| 3389|\n",
            "|FATAL|December|      12|   94|\n",
            "|DEBUG|December|      12|41749|\n",
            "|ERROR|December|      12| 4106|\n",
            "| INFO|December|      12|28874|\n",
            "| WARN|December|      12| 8328|\n",
            "+-----+--------+--------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here i am getting the january but still it is not the right output but why?\n",
        "# is our monthnum is string or integer?\n",
        "# lets try converting the monthnum to integer\n",
        "# so how to convert a string to integer in sql : sol: we can use cast fucntion"
      ],
      "metadata": {
        "id": "xXQ0d03qoW4U"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"\"\"select level,date_format(datetime,'MMMM') as Month, \n",
        "cast(first(date_format(datetime,'M')) as int) as Monthnum, \n",
        "count(*) as count \n",
        "from logging \n",
        "group by level, Month \n",
        "order by Monthnum\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmoI_0ICoyAS",
        "outputId": "974a5537-64fd-4eca-93ec-3dbd380d5bad"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+--------+-----+\n",
            "|level|   Month|Monthnum|count|\n",
            "+-----+--------+--------+-----+\n",
            "|FATAL| January|       1|   94|\n",
            "|DEBUG| January|       1|41961|\n",
            "| INFO| January|       1|29119|\n",
            "|ERROR| January|       1| 4054|\n",
            "| WARN| January|       1| 8217|\n",
            "|DEBUG|February|       2|41734|\n",
            "| WARN|February|       2| 8266|\n",
            "|ERROR|February|       2| 4013|\n",
            "| INFO|February|       2|28983|\n",
            "|FATAL|February|       2|   72|\n",
            "|ERROR|   March|       3| 4122|\n",
            "|FATAL|   March|       3|   70|\n",
            "|DEBUG|   March|       3|41652|\n",
            "| WARN|   March|       3| 8165|\n",
            "| INFO|   March|       3|29095|\n",
            "|FATAL|   April|       4|   83|\n",
            "|DEBUG|   April|       4|41869|\n",
            "|ERROR|   April|       4| 4107|\n",
            "| INFO|   April|       4|29302|\n",
            "| WARN|   April|       4| 8277|\n",
            "+-----+--------+--------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# So now i am getting the output i want in the month column but if you level it is not ordered lets order that to..\n",
        "spark.sql(\"\"\"select level,date_format(datetime,'MMMM') as Month, \n",
        "cast(first(date_format(datetime,'M')) as int) as Monthnum, \n",
        "count(*) as count \n",
        "from logging \n",
        "group by level, Month \n",
        "order by Monthnum, level\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30nZm9lmpCp5",
        "outputId": "e50390c1-81d9-46ba-d74d-9437ccf8a4bd"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+--------+-----+\n",
            "|level|   Month|Monthnum|count|\n",
            "+-----+--------+--------+-----+\n",
            "|DEBUG| January|       1|41961|\n",
            "|ERROR| January|       1| 4054|\n",
            "|FATAL| January|       1|   94|\n",
            "| INFO| January|       1|29119|\n",
            "| WARN| January|       1| 8217|\n",
            "|DEBUG|February|       2|41734|\n",
            "|ERROR|February|       2| 4013|\n",
            "|FATAL|February|       2|   72|\n",
            "| INFO|February|       2|28983|\n",
            "| WARN|February|       2| 8266|\n",
            "|DEBUG|   March|       3|41652|\n",
            "|ERROR|   March|       3| 4122|\n",
            "|FATAL|   March|       3|   70|\n",
            "| INFO|   March|       3|29095|\n",
            "| WARN|   March|       3| 8165|\n",
            "|DEBUG|   April|       4|41869|\n",
            "|ERROR|   April|       4| 4107|\n",
            "|FATAL|   April|       4|   83|\n",
            "| INFO|   April|       4|29302|\n",
            "| WARN|   April|       4| 8277|\n",
            "+-----+--------+--------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# So now it is okay makes some sensable compare to previous one. But, there \n",
        "#is one extra column rite which is monthnum i dont want it lets drop it.\n",
        "# how to drop it just use drop transformation.\n",
        "# lets store it to a dataframe so that we could use drop."
      ],
      "metadata": {
        "id": "0MswGKdxpsBs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = spark.sql(\"\"\"select level,date_format(datetime,'MMMM') as Month, \n",
        "cast(first(date_format(datetime,'M')) as int) as Monthnum, \n",
        "count(*) as count \n",
        "from logging \n",
        "group by level, Month \n",
        "order by Monthnum, level\"\"\")"
      ],
      "metadata": {
        "id": "TXu95NN6qJIR"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = result.drop(\"Monthnum\")"
      ],
      "metadata": {
        "id": "hJgEjF8Dqn1q"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result2.show(40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc7MHHrYqpr1",
        "outputId": "30f3adfb-3bfe-48e4-c362-add1d9ff89d2"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------+-----+\n",
            "|level|   Month|count|\n",
            "+-----+--------+-----+\n",
            "|DEBUG| January|41961|\n",
            "|ERROR| January| 4054|\n",
            "|FATAL| January|   94|\n",
            "| INFO| January|29119|\n",
            "| WARN| January| 8217|\n",
            "|DEBUG|February|41734|\n",
            "|ERROR|February| 4013|\n",
            "|FATAL|February|   72|\n",
            "| INFO|February|28983|\n",
            "| WARN|February| 8266|\n",
            "|DEBUG|   March|41652|\n",
            "|ERROR|   March| 4122|\n",
            "|FATAL|   March|   70|\n",
            "| INFO|   March|29095|\n",
            "| WARN|   March| 8165|\n",
            "|DEBUG|   April|41869|\n",
            "|ERROR|   April| 4107|\n",
            "|FATAL|   April|   83|\n",
            "| INFO|   April|29302|\n",
            "| WARN|   April| 8277|\n",
            "|DEBUG|     May|41785|\n",
            "|ERROR|     May| 4086|\n",
            "|FATAL|     May|   60|\n",
            "| INFO|     May|28900|\n",
            "| WARN|     May| 8403|\n",
            "|DEBUG|    June|41774|\n",
            "|ERROR|    June| 4059|\n",
            "|FATAL|    June|   78|\n",
            "| INFO|    June|29143|\n",
            "| WARN|    June| 8191|\n",
            "|DEBUG|    July|42085|\n",
            "|ERROR|    July| 3976|\n",
            "|FATAL|    July|   98|\n",
            "| INFO|    July|29300|\n",
            "| WARN|    July| 8222|\n",
            "|DEBUG|  August|42147|\n",
            "|ERROR|  August| 3987|\n",
            "|FATAL|  August|   80|\n",
            "| INFO|  August|28993|\n",
            "| WARN|  August| 8381|\n",
            "+-----+--------+-----+\n",
            "only showing top 40 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now it looks good. But make it more sensable because here i am seeing so many fatal etc...all the so many times\n",
        "#what if i can get it just as one like fatal in january and total\n",
        "# Here i want all the jan,feb,marc as columns then i can keep only one repitation of fatal, info and all. like tabular format you can imagine.\n",
        "# So, lets get it done then. how to make rows values to columns then ?\n",
        "# Apache spark provides something called pivot we could use that and get it. Lets do it."
      ],
      "metadata": {
        "id": "wn4kmej6q5sm"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = spark.sql(\"\"\"select level,date_format(datetime,'MMMM') as Month, \n",
        "cast(date_format(datetime,'M') as int) as Monthnum \n",
        "from logging\"\"\").groupBy(\"level\").pivot(\"Monthnum\").count().show(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vAAq5moRxWu6",
        "outputId": "49449cad-a996-4102-f310-eedb1f1cfbc4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "|level|    1|    2|    3|    4|    5|    6|    7|    8|    9|   10|   11|   12|\n",
            "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "| INFO|29119|28983|29095|29302|28900|29143|29300|28993|29038|29018|23301|28874|\n",
            "|ERROR| 4054| 4013| 4122| 4107| 4086| 4059| 3976| 3987| 4161| 4040| 3389| 4106|\n",
            "| WARN| 8217| 8266| 8165| 8277| 8403| 8191| 8222| 8381| 8352| 8226| 6616| 8328|\n",
            "|FATAL|   94|   72|   70|   83|   60|   78|   98|   80|   81|   92|16797|   94|\n",
            "|DEBUG|41961|41734|41652|41869|41785|41774|42085|42147|41433|41936|33366|41749|\n",
            "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SO now it is sensable right and it is easy to see..what is what. \n",
        "# we can still do some optimazation to avoid system to calculate the distinct monthnum operation.\n",
        "# lets do it\n",
        "# here we have monthnames and which are only 12 months it wont be changed so we could hardcode the month names."
      ],
      "metadata": {
        "id": "vzHyECYazFtK"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "month_column = ['January','February','March','April','May','June','July','August','September','October','November','December']"
      ],
      "metadata": {
        "id": "yOE2Zwbz00L_"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = spark.sql(\"\"\"select level, date_format(datetime,'MMMM') as Month \n",
        "from logging\"\"\").groupBy(\"level\").pivot(\"Month\",month_column).count().show(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RpQ7iCn1vBW",
        "outputId": "175eb098-8c94-4ca7-97ae-83134ed53f74"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
            "|level|January|February|March|April|  May| June| July|August|September|October|November|December|\n",
            "+-----+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
            "| INFO|  29119|   28983|29095|29302|28900|29143|29300| 28993|    29038|  29018|   23301|   28874|\n",
            "|ERROR|   4054|    4013| 4122| 4107| 4086| 4059| 3976|  3987|     4161|   4040|    3389|    4106|\n",
            "| WARN|   8217|    8266| 8165| 8277| 8403| 8191| 8222|  8381|     8352|   8226|    6616|    8328|\n",
            "|FATAL|     94|      72|   70|   83|   60|   78|   98|    80|       81|     92|   16797|      94|\n",
            "|DEBUG|  41961|   41734|41652|41869|41785|41774|42085| 42147|    41433|  41936|   33366|   41749|\n",
            "+-----+-------+--------+-----+-----+-----+-----+-----+------+---------+-------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Looks Good! Good Job we could use this to visulize or analyze the why something occured on somemonth \n",
        "# like fatal why there are more FATAL in the month of November and analyse."
      ],
      "metadata": {
        "id": "nmRn2gkp3zqt"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z3M7SBP-4InT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}